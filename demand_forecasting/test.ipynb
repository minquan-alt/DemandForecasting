{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dae785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from typing import Dict, Any\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/guest/DemandForecasting')\n",
    "\n",
    "from data_utils.load_data import load_and_preprocess_data\n",
    "from model.baseline_models import get_baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2634a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 42450000\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = load_and_preprocess_data(\n",
    "    data_path='/home/guest/DemandForecasting/data/imputed_data.csv',\n",
    "    time_encoded=True,\n",
    "    input_len=480,\n",
    "    target_len=112\n",
    ")\n",
    "\n",
    "print(f'Total dataset size: {len(dataset)}')\n",
    "\n",
    "# split train/val/test\n",
    "num_train = int(len(dataset) * 0.99)\n",
    "num_val = int(len(dataset) * (1 - 0.99) / 2)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, list(range(0, num_train)))\n",
    "test_dataset = torch.utils.data.Subset(dataset, list(range(num_train + num_val, len(dataset))))\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fdb4819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset: 42025500\n",
      "Shapes of components of each sample: (torch.Size([480, 11]), torch.Size([112, 10]), torch.Size([112, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Dataset: {len(train_dataset)}\")\n",
    "x, x_dec, y = train_dataset[0]\n",
    "print(f\"Shapes of components of each sample: {x.shape, x_dec.shape, y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wape(y_true, y_pred, eps=1e-8):\n",
    "    return 100 * np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)\n",
    "def compute_daily_wape(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    hours_per_day=16,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: shape (N, T) or (N, T, C)\n",
    "    T phải chia hết cho hours_per_day\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Nếu có channel dim → lấy channel cuối\n",
    "    if y_true.ndim == 3:\n",
    "        y_true = y_true[..., -1]\n",
    "        y_pred = y_pred[..., -1]\n",
    "\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    assert y_true.shape[1] % hours_per_day == 0\n",
    "\n",
    "    num_days = y_true.shape[1] // hours_per_day\n",
    "\n",
    "    # (N, T) → (N, num_days, hours_per_day) → sum theo giờ\n",
    "    y_true_daily = y_true.reshape(-1, num_days, hours_per_day).sum(axis=2)\n",
    "    y_pred_daily = y_pred.reshape(-1, num_days, hours_per_day).sum(axis=2)\n",
    "\n",
    "    return 100 * np.sum(np.abs(y_true_daily - y_pred_daily)) / \\\n",
    "           (np.sum(np.abs(y_true_daily)) + eps)\n",
    "\n",
    "\n",
    "def evaluate_sktime_model(\n",
    "    model_fn,\n",
    "    test_dataset,\n",
    "    window_len=420,\n",
    "    pred_len=112,\n",
    "    max_samples=5000,\n",
    "    hours_per_day=16\n",
    "):\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for i in tqdm(range(min(len(test_dataset), max_samples)), desc=\"Evaluating\"):\n",
    "        x, _, y = test_dataset[i]\n",
    "\n",
    "        series = pd.Series(x[-window_len:, -1].numpy())\n",
    "\n",
    "        model = model_fn()\n",
    "        model.fit(series)\n",
    "\n",
    "        pred = model.predict(np.arange(1, pred_len + 1))\n",
    "\n",
    "        all_predictions.append(pred.values)\n",
    "        all_targets.append(y[:, 0].numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    return {\n",
    "        'WAPE': compute_daily_wape(\n",
    "            all_targets,\n",
    "            all_predictions,\n",
    "            hours_per_day=hours_per_day\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_statsmodels_model(\n",
    "    model_fn,\n",
    "    test_dataset,\n",
    "    window_len=420,\n",
    "    pred_len=112,\n",
    "    max_samples=5000,\n",
    "    hours_per_day=16\n",
    "):\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for i in tqdm(range(min(len(test_dataset), max_samples)), desc=\"Evaluating\"):\n",
    "        x, _, y = test_dataset[i]\n",
    "\n",
    "        series = x[-window_len:, -1].numpy()\n",
    "\n",
    "        try:\n",
    "            model = model_fn(series)\n",
    "            fitted = model.fit(disp=False)\n",
    "            pred = fitted.forecast(steps=pred_len)\n",
    "        except Exception:\n",
    "            pred = np.full(pred_len, series[-1])\n",
    "\n",
    "        all_predictions.append(pred)\n",
    "        all_targets.append(y[:, 0].numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    return {\n",
    "        'WAPE': compute_daily_wape(\n",
    "            all_targets,\n",
    "            all_predictions,\n",
    "            hours_per_day=hours_per_day\n",
    "        )\n",
    "    }\n",
    "\n",
    "def evaluate_dlinear_model_dataset(\n",
    "    model,\n",
    "    test_dataset,\n",
    "    use_decoder=True,\n",
    "    device='cuda',\n",
    "    max_samples=5000,\n",
    "    hours_per_day=16\n",
    "):\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(len(test_dataset), max_samples)), desc=\"Testing DLinear\"):\n",
    "            x, x_dec, y = test_dataset[i]\n",
    "\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            x_dec = x_dec.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "\n",
    "            output = model(x, x_dec)\n",
    "\n",
    "            if not use_decoder:\n",
    "                output = output[:, :, -1:]\n",
    "\n",
    "            all_predictions.append(output.squeeze(0).cpu().numpy())\n",
    "            all_targets.append(y.squeeze(0).cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    return {\n",
    "        'WAPE': compute_daily_wape(\n",
    "            all_targets,\n",
    "            all_predictions,\n",
    "            hours_per_day=hours_per_day\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate_lstm(train_dataset, test_dataset, n_ahead=112, \n",
    "                            num_epochs=3, max_train=10000, max_test=5000, device='cuda'):\n",
    "    \"\"\"Train and evaluate LSTM\"\"\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = get_baseline_model('lstm', input_size=1, hidden_size=64, \n",
    "                              num_layers=2, output_size=1, dropout=0.1)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training\n",
    "    print(\"Training LSTM...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(min(len(train_dataset), max_train)), desc=f\"Epoch {epoch+1}\"):\n",
    "            x, _, y = train_dataset[i]\n",
    "            \n",
    "            # x: (seq_len, features), extract target (last feature) only\n",
    "            x_target = x[:, -1:].unsqueeze(0).to(device)  # (1, seq_len, 1)\n",
    "            y = y.unsqueeze(0).to(device)  # (1, n_ahead, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_target, n_ahead=n_ahead)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/max_train:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Evaluating LSTM...\")\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(len(test_dataset), max_test)), desc=\"Testing\"):\n",
    "            x, _, y = test_dataset[i]\n",
    "            \n",
    "            # Extract target (last feature)\n",
    "            x_target = x[:, -1:].unsqueeze(0).to(device)\n",
    "            pred = model(x_target, n_ahead=n_ahead)\n",
    "            \n",
    "            all_predictions.append(pred.cpu().numpy()[0, :, 0])\n",
    "            all_targets.append(y[:, 0].numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    return {\n",
    "        'Daily_WAPE': compute_daily_wape(\n",
    "            all_targets,\n",
    "            all_predictions,\n",
    "            hours_per_day=16\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0838f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f33c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [00:36<00:00, 136.07it/s]\n"
     ]
    }
   ],
   "source": [
    "results['Naive (Last)'] = evaluate_sktime_model(\n",
    "    lambda: get_baseline_model('naive', strategy='last'),\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e69023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [00:16<00:00, 301.32it/s]\n"
     ]
    }
   ],
   "source": [
    "results['Naive (Mean)'] = evaluate_sktime_model(\n",
    "    lambda: get_baseline_model('naive', strategy='mean'),\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e4e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 28/5000 [00:00<00:17, 279.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [00:17<00:00, 290.14it/s]\n"
     ]
    }
   ],
   "source": [
    "results['Naive (Drift)'] = evaluate_sktime_model(\n",
    "    lambda: get_baseline_model('naive', strategy='drift'),\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "404ffbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [00:01<00:00, 3010.71it/s]\n"
     ]
    }
   ],
   "source": [
    "results['Exponential Smoothing'] = evaluate_statsmodels_model(\n",
    "    get_baseline_model(\n",
    "        'exponential_smoothing',\n",
    "        trend='add',\n",
    "        seasonal='add',\n",
    "        seasonal_periods=16 # theo ngày\n",
    "    ),\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ecc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [00:02<00:00, 2300.41it/s]\n"
     ]
    }
   ],
   "source": [
    "results['ARIMA'] = evaluate_statsmodels_model(\n",
    "    get_baseline_model(\n",
    "        'arima',\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 0, 1, 16)\n",
    "    ),\n",
    "    test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "778263e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, data_type='imputed', use_decoder=True):\n",
    "        # paths\n",
    "        self.data_type = data_type\n",
    "        self.data_path = f'/home/guest/DemandForecasting/data/{data_type}_data.csv'\n",
    "        self.save_path = f'/home/guest/DemandForecasting/demand_forecasting/checkpoints/{data_type}_{\"decoder\" if use_decoder else \"no_decoder\"}/'\n",
    "        self.log_path = f'/home/guest/DemandForecasting/demand_forecasting/logs/{data_type}_{\"decoder\" if use_decoder else \"no_decoder\"}/'\n",
    "        \n",
    "        # model parameters\n",
    "        self.model = 'dlinear'\n",
    "        self.patience = 5\n",
    "        self.enable_scheduler = True\n",
    "        self.seq_len = 480\n",
    "        self.pred_len = 112\n",
    "        self.enc_in = 11\n",
    "        self.dec_in = 10\n",
    "        self.use_decoder = use_decoder\n",
    "        self.individual = True\n",
    "        \n",
    "        # training parameters\n",
    "        self.batch_size = 1024\n",
    "        self.lr = 0.001\n",
    "        self.epochs = 20\n",
    "        self.train_ratio = 0.99\n",
    "        \n",
    "configs = Config(data_type='imputed', use_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a8b696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (decompsition): series_decomp(\n",
       "    (moving_avg): moving_avg(\n",
       "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (dec_projection): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (Linear_Seasonal): ModuleList(\n",
       "    (0-10): 11 x Linear(in_features=480, out_features=112, bias=True)\n",
       "  )\n",
       "  (Linear_Trend): ModuleList(\n",
       "    (0-10): 11 x Linear(in_features=480, out_features=112, bias=True)\n",
       "  )\n",
       "  (Linear_Decoder): ModuleList(\n",
       "    (0-10): 11 x Linear(in_features=480, out_features=112, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.dlinear import Model\n",
    "import os\n",
    "\n",
    "model = Model(configs)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "ckpt_path = '/home/guest/DemandForecasting/demand_forecasting/checkpoints/imputed_decoder/best_dlinear_model.pth'\n",
    "\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "830d054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing DLinear: 100%|██████████| 5000/5000 [00:02<00:00, 1936.11it/s]\n"
     ]
    }
   ],
   "source": [
    "results['DLinear (Proposed)'] = evaluate_dlinear_model_dataset(\n",
    "    model,\n",
    "    test_dataset,\n",
    "    use_decoder=configs.use_decoder,\n",
    "    device=device,\n",
    "    max_samples=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f8be591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Naive (Last)': {'WAPE': np.float32(93.07702)},\n",
       " 'Naive (Mean)': {'WAPE': np.float32(34.818256)},\n",
       " 'Naive (Drift)': {'WAPE': np.float64(106.6897770197577)},\n",
       " 'Exponential Smoothing': {'WAPE': np.float32(93.07702)},\n",
       " 'ARIMA': {'WAPE': np.float32(93.07702)},\n",
       " 'DLinear (Proposed)': {'WAPE': np.float32(31.939238)}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
